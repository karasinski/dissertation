
@article{hart_nasa-task_2006,
	title = {{NASA}-{Task} {Load} {Index} ({NASA}-{TLX}); 20 {Years} {Later}},
	volume = {50},
	issn = {1541-9312},
	url = {https://doi.org/10.1177/154193120605000909},
	doi = {10.1177/154193120605000909},
	abstract = {NASA-TLX is a multi-dimensional scale designed to obtain workload estimates from one or more operators while they are performing a task or immediately afterwards. The years of research that preceded subscale selection and the weighted averaging approach resulted in a tool that has proven to be reasonably easy to use and reliably sensitive to experimentally important manipulations over the past 20 years. Its use has spread far beyond its original application (aviation), focus (crew complement), and language (English). This survey of 550 studies in which NASA-TLX was used or reviewed was undertaken to provide a resource for a new generation of users. The goal was to summarize the environments in which it has been applied, the types of activities the raters performed, other variables that were measured that did (or did not) covary, methodological issues, and lessons learned},
	language = {en},
	number = {9},
	urldate = {2019-04-23},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Hart, Sandra G.},
	month = oct,
	year = {2006},
	pages = {904--908},
	file = {SAGE PDF Full Text:C\:\\Users\\John Karasinski\\Zotero\\storage\\UINCETRU\\Hart - 2006 - Nasa-Task Load Index (NASA-TLX)\; 20 Years Later.pdf:application/pdf}
}

@article{sigrist_augmented_2013,
	title = {Augmented visual, auditory, haptic, and multimodal feedback in motor learning: {A} review},
	volume = {20},
	issn = {1531-5320},
	shorttitle = {Augmented visual, auditory, haptic, and multimodal feedback in motor learning},
	url = {https://doi.org/10.3758/s13423-012-0333-8},
	doi = {10.3758/s13423-012-0333-8},
	abstract = {It is generally accepted that augmented feedback, provided by a human expert or a technical display, effectively enhances motor learning. However, discussion of the way to most effectively provide augmented feedback has been controversial. Related studies have focused primarily on simple or artificial tasks enhanced by visual feedback. Recently, technical advances have made it possible also to investigate more complex, realistic motor tasks and to implement not only visual, but also auditory, haptic, or multimodal augmented feedback. The aim of this review is to address the potential of augmented unimodal and multimodal feedback in the framework of motor learning theories. The review addresses the reasons for the different impacts of feedback strategies within or between the visual, auditory, and haptic modalities and the challenges that need to be overcome to provide appropriate feedback in these modalities, either in isolation or in combination. Accordingly, the design criteria for successful visual, auditory, haptic, and multimodal feedback are elaborated.},
	language = {en},
	number = {1},
	urldate = {2019-04-23},
	journal = {Psychonomic Bulletin \& Review},
	author = {Sigrist, Roland and Rauter, Georg and Riener, Robert and Wolf, Peter},
	month = feb,
	year = {2013},
	keywords = {Augmented extrinsic feedback, Feedback strategy, Skill learning and automaticity, Unimodal feedback},
	pages = {21--53},
	file = {Springer Full Text PDF:C\:\\Users\\John Karasinski\\Zotero\\storage\\GS3FC88L\\Sigrist et al. - 2013 - Augmented visual, auditory, haptic, and multimodal.pdf:application/pdf}
}

@inproceedings{karasinski_evaluating_2019,
	address = {San Diego, California},
	title = {Evaluating {Augmented} {Reality} in a {Three}-{Axis} {Manual} {Tracking} {Task}},
	isbn = {978-1-62410-578-4},
	url = {https://arc.aiaa.org/doi/10.2514/6.2019-1227},
	doi = {10.2514/6.2019-1227},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {{AIAA} {Scitech} 2019 {Forum}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Karasinski, John and Robinson, Stephen},
	month = jan,
	year = {2019},
	file = {Karasinski and Robinson - 2019 - Evaluating Augmented Reality in a Three-Axis Manua.pdf:C\:\\Users\\John Karasinski\\Zotero\\storage\\Q2SVMQ23\\Karasinski and Robinson - 2019 - Evaluating Augmented Reality in a Three-Axis Manua.pdf:application/pdf}
}

@mastersthesis{karasinski_real-time_2016,
	address = {United States -- California},
	title = {Real-{Time} {Performance} {Feedback} for the {Manual} {Control} of {Spacecraft}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://search.proquest.com/docview/1872339216/abstract/1D8D65D9AB6E4725PQ/1},
	abstract = {Real-time performance metrics were developed to quantify workload, situational awareness, and manual task performance for use as visual feedback to pilots of aerospace vehicles. Results from prior lunar lander experiments with variable levels of automation were replicated and extended to provide insights for the development of real-time metrics. Increased levels of automation resulted in increased flight performance, lower workload, and increased situational awareness. Automated Speech Recognition (ASR) was employed to detect verbal callouts as a limited measure of subjects' situational awareness. A one-dimensional manual tracking task and simple instructor-model visual feedback scheme was developed. This feedback was indicated to the operator by changing the color of a guidance element on the primary flight display, similar to how a flight instructor points out elements of a display to a student pilot. Experiments showed that for this low-complexity task, visual feedback did not change subject performance, but did increase the subjects' measured workload. Insights gained from these experiments were applied to a Simplified Aid for EVA Rescue (SAFER) inspection task. The effects of variations of an instructor-model performance-feedback strategy on human performance in a novel SAFER inspection task were investigated. Real-time feedback was found to have a statistically significant effect of improving subject performance and decreasing workload in this complicated four degree of freedom manual control task with two secondary tasks.},
	language = {English},
	urldate = {2019-04-23},
	school = {University of California, Davis},
	author = {Karasinski, John Austin},
	year = {2016},
	keywords = {Applied sciences, Feedback, Human factors, Manual control},
	file = {Full Text PDF:C\:\\Users\\John Karasinski\\Zotero\\storage\\NNKEL3HB\\Karasinski - 2016 - Real-Time Performance Feedback for the Manual Cont.pdf:application/pdf}
}

@inproceedings{karasinski_development_2016,
	title = {Development of real-time performance metrics for manually-guided spacecraft operations},
	doi = {10.1109/AERO.2016.7500734},
	abstract = {Real-time performance metrics were developed to quantify workload, situational awareness, and manual task performance for use as visual feedback to pilots of aerospace vehicles. Results from prior lunar lander experiments were replicated and extended to provide insights for the development of real-time metrics. Automated Speech Recognition (ASR) was employed to detect verbal callouts and measure subjects' situational awareness. ASR achieved an F-measure of 0.8, despite large variances within subjects. A one-dimensional manual tracking task and simple instructor-model visual feedback scheme was developed. Experiments showed that for this low-complexity task, visual feedback did not change subject performance, but did increase the subjects' measured workload.},
	booktitle = {2016 {IEEE} {Aerospace} {Conference}},
	author = {Karasinski, J. A. and Robinson, S. K. and Duda, K. R. and Prasov, Z.},
	month = mar,
	year = {2016},
	keywords = {aerospace computing, aerospace vehicles, ASR, automated speech recognition, F-measure, human factors, Human factors, instructor-model visual feedback scheme, low-complexity task, lunar lander experiments, manually-guided spacecraft operations, Manuals, Measurement, Moon, one-dimensional manual tracking task, Probes, real-time performance metrics development, Real-time systems, situational awareness, space vehicles, speech recognition, Speech recognition, verbal callouts, visual feedback},
	pages = {1--9},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\John Karasinski\\Zotero\\storage\\IXQ8DWSD\\7500734.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\John Karasinski\\Zotero\\storage\\Z3D7WYLL\\Karasinski et al. - 2016 - Development of real-time performance metrics for m.pdf:application/pdf}
}

@inproceedings{karasinski_real-time_2017,
	address = {Grapevine, Texas},
	title = {Real-{Time} {Performance} {Feedback} in a {Manually}-{Controlled} {Spacecraft} {Inspection} {Task}},
	isbn = {978-1-62410-451-0},
	url = {http://arc.aiaa.org/doi/10.2514/6.2017-1314},
	doi = {10.2514/6.2017-1314},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {{AIAA} {Modeling} and {Simulation} {Technologies} {Conference}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Karasinski, John and Robinson, Stephen and Handley, Patrick and Duda, Kevin},
	month = jan,
	year = {2017},
	file = {Karasinski et al. - 2017 - Real-Time Performance Feedback in a Manually-Contr.pdf:C\:\\Users\\John Karasinski\\Zotero\\storage\\TIXQ3PBF\\Karasinski et al. - 2017 - Real-Time Performance Feedback in a Manually-Contr.pdf:application/pdf}
}
